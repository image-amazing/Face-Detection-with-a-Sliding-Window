<html>
<head>
<title>Face Detection Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Wei Yang Quek (903164424) </h1>
</div>
</div>
<div class="container">


    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/ExtraScenes/detections_4495_2015_class_easy.jpg.png"/>
    
    <h2> Project 5 / Face Detection with a Sliding Window</h2>

<div style="float: right; padding: 20px">
<img src="images/Training/HogSize3/HOG.PNG" width="90%"/>
<p style="font-size: 14px" align="center">HOG image of face detected. (HOG cell Size = 3)</p>
</div>

<p> 	In this project, the Dalal-Triggs Sliding Window Detector for Face Detection was implemented. This consists of the following parts:</p>

    <a name="content">
        <h2>Content</h2>
        <ol>
        <li><a href="#data">Dataset</a></li>
        <li><a href="#pos">Description and Retrieval of Positive Features</a></li>
        <li><a href="#neg">Description and Retrieval of Negative Features</a></li>
        <li><a href="#svm">Training of Features using SVM</a></li>
        <li><a href="#res-train">Results of Training</a></li>
        <li><a href="#test">Detection Algorithm on Test Images & Results</a></li>
        <li><a href="#extra">Extra Credits</a></li>
        <li><a href="#extra-scene">Extra Scenes</a></li>
        </ol>
    </a>
    
    <a name="data"><h2>Dataset</h2></a>
    The dataset used comes from the following sources:
        <ul>
        <li>Positive Features: <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/">Caltech Web Face Project</a> (6713 36x36 faces)</li>
        <li>Negative Features: Wu et. al. and <a href="http://groups.csail.mit.edu/vision/SUN/">SUN Scene Database</a></li>
        <li>Testing Data: CMU+MIT test Set (130 images with 511 faces)</li>
        </ul>

    <a href="#content">Back to Contents</a>
    
    
    <a name="pos"><h2>Description and Retrieval of Positive Features</h2></a>

        The detection of positive features from the training set of faces was relatively straightforward, since each image was already a pre-cropped 36x36 face. First, the images were read in individually. Each Image was then normalised and its Histogram of Oriented Gradients (HOG) was calculated with each HOG cell size as 6. As all the faces were 36x36 pixels in dimension, it resulted in 6x6 HOG matrix over 31 dimensions. I then converted this 6x6x31 matrix to a string of 1116 features and saved it into a matrix. This was repeated for all images which resulted in a N x 1116 Matrix (feature_pos), where N is the number of images.
        The result from the first conversion of Argentina.jpg is shown below:
        
    <img src=""/>
        
    <a href="#content">Back to Contents</a>

    <a name="neg"><h2>Description and Retrieval of Negative Features</h2></a>

       <p> For the Negative Features, the dataset used contained images of different sizes and objects. While this complicate things a bit, it was still pretty straightforward. First, with the given number of negative features, an average number of features per images was calculated. I then added addtional features to be obtained per image as some images will not be able to have that many features to be retrieved, especially when I raised the number of negative features to 20,000.
    </p>
    
    <p>
        The image was then converted to a HOG image and the same process was then repeated in retrieving the values of each feature and saving them to a matrix. The resultant matrix is a N x 1116 matrix where N is the number of features.
    </p>
        
    <a href="#content">Back to Contents</a>    
    
     <a name="svm"><h2>Training of Feature using SVM</h2></a>

       <p>
           With both the negative and positive examples ready, I transposed and combined them to a variable called dataset, created a matrix and saved the labels corresponding to each feature to be trained, and with a lambda value of 0.0001, passed the data in to vl_svmtrain to produce the model for future training. The code snippet for this is shown below:
    </p>
    
    <pre><code>
    num_positive_examples = size(features_pos,1);
    num_negative_examples = size(features_neg,1);

    dataset = [features_pos' features_neg'];
    labels(1,1:num_positive_examples) = 1;
    labels(1,num_positive_examples+1:num_positive_examples+num_negative_examples) = -1;
    lambda = 0.0001;
    [w b] = vl_svmtrain(dataset,labels,lambda);
    </code></pre>
        
    <a href="#content">Back to Contents</a>    
    
     <a name="res-train"><h2>Result of Training</h2></a>

       <p>
           With 10,000 negative features, I got a classfier accuracy of: 0.999.
           
           However, when the number of negative features was increased to 20,000, the classifier accuracy increased to 1.00 (TP: 0.01, FP: 0, TN: 0.99, FN:0). The results are shown below for a HOG cell size of 6:
    </p>
    
    <center><p>
    <img src="images/Training/20kNeg/HOG.PNG" width="30%"/>
    <img src="images/Training/20kNeg/PosNegTrainingGraph.PNG" width="30%"/>
    </p></center>
    
       <p>
          And the results below are obtained when trained with a HOG cell size of 3 (also 20,000 negative features).
    </p>
    
    <center><p>
    <img src="images/Training/HogSize3/HOG.PNG" width="30%"/>
    <img src="images/Training/HogSize3/Curve.PNG" width="30%"/>
    </p></center>
    
        
    <a href="#content">Back to Contents</a>        
    
    
      <a name="test"><h2>Detection Algorithm for Test Image</h2></a>

    <h4>Single-Scale</h4>
       <p>
           First, I created a program to detect features for a fixed scale size of 1. For each test image, I converted the image to a HOG image. I then take a segment equal to the template size that was trained in the training phase. Here, I used 36x36 which translates to 6x6 HOG cells. For each of the segment, I saved the values into a 1 x 1116 dimensional matrix similar to the training phase.
    </p>
    
    <p>
        I then Calculated the confidence of each of this matrix using the following formula:
        <pre><code>tempConf = dot(w,tempFeature') + b;</code></pre>
        If the confidence is above some threshold (here I used -1), I save the bounding box corresponding to the segment being detected.
    </p>
    
    <p>Finally, non-maximal suppression was done to remove detections that are too close together, leaving only the more confident features.</p>
    
    <h4>Results for Single-Scale</h4>
    
    For the single-scale algorithm, I got quite a decent Average Precision of 0.394. Screenshots of the results can be seen below.

    <center><p>
    <img src="images/Single-Scale/step1T-1/PRC.JPG" width="40%"/>
    <img src="images/Single-Scale/step1T-1/Plot1.JPG" width="40%"/>
    <img src="images/Single-Scale/step1T-1/Argentina.JPG" width="40%"/>
    </p></center>
    
    <h4>Multi-Scale</h4>
        
    <p>
        I then modified my code to detect faces at multiple scales.
    </p>
    
    <p>With Each Image that was to be tested, I resized the image to the current scale size starting from a scale of 2. As per above, I got the HOG features for that image at that size and passed a sliding window of the same size as the template through each HOG cell of the HOG image.
    
    </p>
    
    <p>The result is tested using the model trained and the confidence is obtained. As with the single scale algorithm, the confidence is checked against a threshold and if its above it, the bounding box is saved. However, there was an adjustment that had to be done here. Since the image was scaled, the bounding box's size had to be scaled back to the original image size. This was done using the following code:</p>
    
    <pre><code>
     if(tempConf > threshold_confidence)
                    
        cur_x_min = ((x-1)*hogCellSize / scale)+1;
        cur_y_min = ((y-1)*hogCellSize / scale)+1;
        cur_x_max = ((x-1)+noOfHogPerTemplate)*hogCellSize / scale;
        cur_y_max = ((y-1)+noOfHogPerTemplate)*hogCellSize / scale;

        noOfFeaturesPerImage = noOfFeaturesPerImage+1;
        facesPerImage(noOfFeaturesPerImage,:) = tempFeature;
        cur_confidences(noOfFeaturesPerImage,1) = tempConf;
%                 cur_bboxes(noOfFeaturesPerImage,:) = [cur_y_min, cur_x_min, (cur_y_min + templateSize-1), (cur_x_min +templateSize-1)];
        cur_bboxes(noOfFeaturesPerImage,:) = [cur_x_min, cur_y_min, cur_x_max, cur_y_max];
    end
    </code></pre>
    <p>
        The result of all the bounding box was then similarly passed through a non-maximal suppression phase. The scale is then changed by a factor (e.g. 0.8) and the process is repeated for different scales of that same image until the image is just above the template size. All images to be tested are then passed through the same steps.
    </p>
    <h4>Results</h4>
    <p>
        With the following Conditions:
    <ul>
    <li>Threshold: -0.5</li>
    <li>Scale Start: 1</li>
    <li>Scale Step: 0.8 (i.e. scale = 1, 0.8, 0.64, ...)</li>
    <li>Hog Cell Size: 6</li>
    </ul>
    I got an Average Precision of 0.821 with a runtime of 1458 seconds.
    </p>

    <center><p>
    <img src="images/Multi-Scale/start1step0.8/PRC.JPG" width="40%"/>
    <img src="images/Multi-Scale/start1step0.8/Plot1.JPG" width="40%"/>
    <img src="images/Multi-Scale/start1step0.8/Argentina.JPG" width="40%"/>
    <img src="images/Multi-Scale/start1step0.8/Addams.JPG" width="40%"/>

    </p></center>
    
    <a href="#content">Back to Contents</a> 
    
    <a name="extra"><h2>Extra Credits</h2></a>
    
    <h3>Reducing Hog Cell Size</h3>
    <p> To test the effects of a reduced Hog cell size, I decrease the HOG cell size to 3 and ran the multi-scale program. The classifier accuracy remained at 1.0 (TP:0.251, FP: 0, TN: 0.748, FN: 0). The HOG and the result of the classifier for this version is shown below:</p>
            <center><p>

    <img src="images/Training/HogSize3/HOG.PNG" width="40%"/>
    <img src="images/Training/HogSize3/Curve.PNG" width="40%"/>
    </p></center>
    <p>With such a classifier and Hog Cell Size, the average Precision increased to 0.925. The results obtained from this is shown below: </p>
    
        <center><p>
    <img src="images/Training/HogSize3/PRC.JPG" width="40%"/>
    <img src="images/Multi-Scale/HOGCellSize3/visualizations/detections_Argentina.jpg.png" width="40%"/>

    </p></center>
    
     <h3>Hard Negative Mining</h3>
    <p> In the Paper <a href="http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">"Histogram of Oriented Gradients for Human Detection"</a> by Dalal and Triggs, a hard negative mining was done. In this process, once all the positive and negative examples were trained and a model created, the negative examples were passed through the detection algorithm and features that were detected were added to the 'negative pile'. Together with the original positive and negative training examples, the newly detected negative features are then passed through the svm function to create a new, more robust model.</p>
         <p>I implemented this via the following code:
         
         <pre><code>
     [features, bboxes_1, confidences_1, image_ids_1] = run_detector(non_face_scn_path, w, b, feature_params);

    num_positive_examples = size(features_pos,1);
    features_neg = [features_neg; features];
    num_negative_examples = size(features_neg,1);

    dataset = [features_pos' features_neg'];
    labels(1,1:num_positive_examples) = 1;
    labels(1,num_positive_examples+1:num_positive_examples+num_negative_examples) = -1;
    lambda = 0.0001;
    [w b] = vl_svmtrain(dataset,labels,lambda);
         </code></pre>
         
         </p>

        <p>Initially, I used a threshold of -0.7 for both the hard negative mining and testing under the following conditions:
        <ul>
    <li>Hard Mining Threshold: -0.7</li>
    <li>Testing Threshold: -0.7</li>
    <li>Scale Start: 1</li>
    <li>Scale Step: 0.8 (i.e. scale = 1, 0.8, 0.64, ...)</li>
    <li>Hog Cell Size: 6</li>

</ul>

<p> This resulted in a drop of Average Precision of 0.649. This could be due to a decrease in the number of True Postitives although there was an apparent decrease in False Positives as well. This can be seen from the decrease in Red Boundary Rectangles in the following image:</p>

<center><p>
    <img src="images/Multi-Scale/HardNeg/T-0.7/visualizations/detections_Argentina.jpg.png" width="40%"/>
    <img src="images/Multi-Scale/HardNeg/T-0.7/visualizations/detections_addams-family.jpg.png" width="40%"/><br/>
    </p></center>   
<p>When the Threshold was increased to -0.5, there was an increase in Average Precision as well (AP = 0.730), however there was an increase in False Positives as well. However, the number of False Positive, especially as seen from the Argentina Picture was still lesser than without Hard Mining.</p>
     <center><p>
     <img src="images/Multi-Scale/HardNeg/T-0.5/visualizations/detections_Argentina.jpg.png" width="40%"/>
    <img src="images/Multi-Scale/HardNeg/T-0.5/visualizations/detections_addams-family.jpg.png" width="40%"/><br/>
    </p></center>      

<p>Theoretically, a more restrictive threshold for Hard Mining should reduce the number of False Positives, but a more restrictive threshold for the testing phase would lead to lesser True Positives as well. To allow for a more optimal condition, I edited my code for run_detector such that the hard mining threshold could be different from the testing threshold.</p>
    <p>With the following conditions for the test,
        <ul>
    <li>Hard Mining Threshold: 0</li>
    <li>Testing Threshold: -0.5</li>
    <li>Scale Start: 1</li>
    <li>Scale Step: 0.8 (i.e. scale = 1, 0.8, 0.64, ...)</li>
    <li>Hog Cell Size: 6</li>

</ul>
        
        I got an Average Precision of 0.879, an increase in accuracy compared to the test for Hog Cell Size 6 with no Hard Negative Mining with an AP of 0.821. When the step size was increased from 0.8 to 0.9, the accuracy increased further to 0.895. The Results are shown below </p>
    
    
        <center><p>
    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/average_precision.png" width="40%"/><br/>
    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/detections_addams-family.jpg.png" width="40%"/>
    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/detections_Argentina.jpg.png" width="40%"/>

    </p></center>   
    
    <p>We can see an apparent increase in Average Precision when a round of Hard Mining was done although there were more False Positives as well. </p>
    
        <a href="#content">Back to Contents</a> 
<a name="extra-scene"><h2>Extra Scenes</h2></a>
<p>These are the results observed using Hard Mining and Multi-scale. I used a Hog Cell Size of 6 with thresholds 0 for the hard mining and -0.5 for the testing, and a multi-scale with a scale starting at 1 and decreasing to 0.9 of the size at every iteration.</p>
            <center><p>
    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/ExtraScenes/detections_4495_2015_class_easy.jpg.png"/>
    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/ExtraScenes/detections_cs143_2011_class_easy.jpg.png" width="80%"/>
    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/ExtraScenes/detections_cs143_2011_class_hard.jpg.png"width="80%"/>
    <img src="images/Multi-Scale/HardNeg/T0-0.5/Step0.9/ExtraScenes/detections_cs143_2013_class_hard_01.jpg.png"/>

    </p></center>  
<p>We can see that that results are decent with most of the faces detected and not a huge amount of false positives. Even when faces were hidden, whatever clues were provided by the parts of the faces that were shown could be detected as a face in right proportions as well.</p>
        <a href="#content">Back to Contents</a> 
    
</body>
</html>
